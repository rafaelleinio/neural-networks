{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# python utils\n",
    "import itertools\n",
    "import inspect\n",
    "\n",
    "# pre-processing and exploring data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import statistics\n",
    "\n",
    "# viz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# model building\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "from pyod.utils.data import generate_data\n",
    "from keras.losses import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# artificial data with outliers\n",
    "\n",
    "contamination = 0.1  # percentage of outliers\n",
    "n_train = 700  # number of training points\n",
    "n_test = 700  # number of testing points\n",
    "n_features = 18 # Number of features\n",
    "\n",
    "X_data, y_data = generate_data(\n",
    "    n_train=n_train, n_test=n_test,\n",
    "    n_features= n_features, \n",
    "    train_only=True,\n",
    "    contamination=contamination,random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.892732</td>\n",
       "      <td>2.805184</td>\n",
       "      <td>2.550995</td>\n",
       "      <td>3.552799</td>\n",
       "      <td>3.535617</td>\n",
       "      <td>2.603377</td>\n",
       "      <td>3.009781</td>\n",
       "      <td>1.602563</td>\n",
       "      <td>3.716597</td>\n",
       "      <td>3.618090</td>\n",
       "      <td>3.594025</td>\n",
       "      <td>1.740538</td>\n",
       "      <td>2.791833</td>\n",
       "      <td>3.001320</td>\n",
       "      <td>3.252642</td>\n",
       "      <td>3.180136</td>\n",
       "      <td>3.823225</td>\n",
       "      <td>2.036110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.873729</td>\n",
       "      <td>2.591260</td>\n",
       "      <td>3.120523</td>\n",
       "      <td>3.344853</td>\n",
       "      <td>3.821352</td>\n",
       "      <td>2.707572</td>\n",
       "      <td>3.420944</td>\n",
       "      <td>1.867794</td>\n",
       "      <td>2.885903</td>\n",
       "      <td>3.659853</td>\n",
       "      <td>2.752102</td>\n",
       "      <td>3.210260</td>\n",
       "      <td>3.652755</td>\n",
       "      <td>3.651733</td>\n",
       "      <td>3.538190</td>\n",
       "      <td>2.923924</td>\n",
       "      <td>3.077710</td>\n",
       "      <td>2.798864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.524455</td>\n",
       "      <td>4.489828</td>\n",
       "      <td>3.047481</td>\n",
       "      <td>2.647043</td>\n",
       "      <td>3.022520</td>\n",
       "      <td>1.707063</td>\n",
       "      <td>3.154401</td>\n",
       "      <td>2.440974</td>\n",
       "      <td>2.914762</td>\n",
       "      <td>3.011396</td>\n",
       "      <td>3.470705</td>\n",
       "      <td>3.134136</td>\n",
       "      <td>3.524040</td>\n",
       "      <td>2.099103</td>\n",
       "      <td>2.126418</td>\n",
       "      <td>2.937117</td>\n",
       "      <td>2.658385</td>\n",
       "      <td>2.909886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.220593</td>\n",
       "      <td>2.977872</td>\n",
       "      <td>3.352517</td>\n",
       "      <td>3.963114</td>\n",
       "      <td>2.392945</td>\n",
       "      <td>2.956167</td>\n",
       "      <td>3.191898</td>\n",
       "      <td>2.870083</td>\n",
       "      <td>3.644170</td>\n",
       "      <td>1.504256</td>\n",
       "      <td>4.265287</td>\n",
       "      <td>2.288016</td>\n",
       "      <td>3.132026</td>\n",
       "      <td>3.439118</td>\n",
       "      <td>2.510588</td>\n",
       "      <td>3.287913</td>\n",
       "      <td>3.438811</td>\n",
       "      <td>3.326202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.422843</td>\n",
       "      <td>4.251105</td>\n",
       "      <td>3.141422</td>\n",
       "      <td>2.281768</td>\n",
       "      <td>3.393792</td>\n",
       "      <td>3.024621</td>\n",
       "      <td>3.289367</td>\n",
       "      <td>0.779542</td>\n",
       "      <td>3.823192</td>\n",
       "      <td>3.095105</td>\n",
       "      <td>3.102520</td>\n",
       "      <td>2.732004</td>\n",
       "      <td>3.478154</td>\n",
       "      <td>3.613712</td>\n",
       "      <td>3.168760</td>\n",
       "      <td>3.867359</td>\n",
       "      <td>3.049750</td>\n",
       "      <td>2.750779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>1.281831</td>\n",
       "      <td>1.062482</td>\n",
       "      <td>-0.439999</td>\n",
       "      <td>0.445105</td>\n",
       "      <td>1.399909</td>\n",
       "      <td>-0.780047</td>\n",
       "      <td>0.327582</td>\n",
       "      <td>-1.303468</td>\n",
       "      <td>2.596278</td>\n",
       "      <td>-1.252098</td>\n",
       "      <td>-2.640238</td>\n",
       "      <td>-2.654077</td>\n",
       "      <td>2.520779</td>\n",
       "      <td>-1.295039</td>\n",
       "      <td>-2.593610</td>\n",
       "      <td>2.974788</td>\n",
       "      <td>-0.992141</td>\n",
       "      <td>-0.669199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>1.108874</td>\n",
       "      <td>0.942599</td>\n",
       "      <td>-1.632943</td>\n",
       "      <td>-2.813374</td>\n",
       "      <td>-1.537338</td>\n",
       "      <td>-1.230528</td>\n",
       "      <td>2.424851</td>\n",
       "      <td>2.072350</td>\n",
       "      <td>2.839880</td>\n",
       "      <td>-0.686806</td>\n",
       "      <td>-0.234186</td>\n",
       "      <td>-0.709740</td>\n",
       "      <td>-0.640154</td>\n",
       "      <td>1.011155</td>\n",
       "      <td>-1.067273</td>\n",
       "      <td>1.911158</td>\n",
       "      <td>2.666463</td>\n",
       "      <td>0.780191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>0.667630</td>\n",
       "      <td>-0.741268</td>\n",
       "      <td>0.492865</td>\n",
       "      <td>-1.617740</td>\n",
       "      <td>2.404900</td>\n",
       "      <td>-2.150404</td>\n",
       "      <td>-1.008687</td>\n",
       "      <td>-1.290717</td>\n",
       "      <td>-1.428679</td>\n",
       "      <td>-2.289012</td>\n",
       "      <td>-1.264462</td>\n",
       "      <td>-2.905294</td>\n",
       "      <td>1.925509</td>\n",
       "      <td>-2.995778</td>\n",
       "      <td>0.159113</td>\n",
       "      <td>2.839631</td>\n",
       "      <td>0.013598</td>\n",
       "      <td>0.681762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>-2.522522</td>\n",
       "      <td>-0.878053</td>\n",
       "      <td>-1.548773</td>\n",
       "      <td>2.826363</td>\n",
       "      <td>0.922174</td>\n",
       "      <td>2.248154</td>\n",
       "      <td>-1.393076</td>\n",
       "      <td>2.926527</td>\n",
       "      <td>0.494016</td>\n",
       "      <td>2.740795</td>\n",
       "      <td>-0.361400</td>\n",
       "      <td>1.060685</td>\n",
       "      <td>1.927476</td>\n",
       "      <td>2.978506</td>\n",
       "      <td>-0.538000</td>\n",
       "      <td>1.899725</td>\n",
       "      <td>-0.097139</td>\n",
       "      <td>-1.321670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>0.287430</td>\n",
       "      <td>1.219600</td>\n",
       "      <td>0.211036</td>\n",
       "      <td>1.135554</td>\n",
       "      <td>-1.325231</td>\n",
       "      <td>2.356455</td>\n",
       "      <td>2.886383</td>\n",
       "      <td>-1.714145</td>\n",
       "      <td>-0.308792</td>\n",
       "      <td>2.168168</td>\n",
       "      <td>-2.845136</td>\n",
       "      <td>2.784231</td>\n",
       "      <td>0.412130</td>\n",
       "      <td>0.299064</td>\n",
       "      <td>-1.565722</td>\n",
       "      <td>0.025623</td>\n",
       "      <td>1.603908</td>\n",
       "      <td>1.013197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    3.892732  2.805184  2.550995  3.552799  3.535617  2.603377  3.009781   \n",
       "1    2.873729  2.591260  3.120523  3.344853  3.821352  2.707572  3.420944   \n",
       "2    3.524455  4.489828  3.047481  2.647043  3.022520  1.707063  3.154401   \n",
       "3    3.220593  2.977872  3.352517  3.963114  2.392945  2.956167  3.191898   \n",
       "4    2.422843  4.251105  3.141422  2.281768  3.393792  3.024621  3.289367   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "695  1.281831  1.062482 -0.439999  0.445105  1.399909 -0.780047  0.327582   \n",
       "696  1.108874  0.942599 -1.632943 -2.813374 -1.537338 -1.230528  2.424851   \n",
       "697  0.667630 -0.741268  0.492865 -1.617740  2.404900 -2.150404 -1.008687   \n",
       "698 -2.522522 -0.878053 -1.548773  2.826363  0.922174  2.248154 -1.393076   \n",
       "699  0.287430  1.219600  0.211036  1.135554 -1.325231  2.356455  2.886383   \n",
       "\n",
       "            7         8         9        10        11        12        13  \\\n",
       "0    1.602563  3.716597  3.618090  3.594025  1.740538  2.791833  3.001320   \n",
       "1    1.867794  2.885903  3.659853  2.752102  3.210260  3.652755  3.651733   \n",
       "2    2.440974  2.914762  3.011396  3.470705  3.134136  3.524040  2.099103   \n",
       "3    2.870083  3.644170  1.504256  4.265287  2.288016  3.132026  3.439118   \n",
       "4    0.779542  3.823192  3.095105  3.102520  2.732004  3.478154  3.613712   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "695 -1.303468  2.596278 -1.252098 -2.640238 -2.654077  2.520779 -1.295039   \n",
       "696  2.072350  2.839880 -0.686806 -0.234186 -0.709740 -0.640154  1.011155   \n",
       "697 -1.290717 -1.428679 -2.289012 -1.264462 -2.905294  1.925509 -2.995778   \n",
       "698  2.926527  0.494016  2.740795 -0.361400  1.060685  1.927476  2.978506   \n",
       "699 -1.714145 -0.308792  2.168168 -2.845136  2.784231  0.412130  0.299064   \n",
       "\n",
       "           14        15        16        17  \n",
       "0    3.252642  3.180136  3.823225  2.036110  \n",
       "1    3.538190  2.923924  3.077710  2.798864  \n",
       "2    2.126418  2.937117  2.658385  2.909886  \n",
       "3    2.510588  3.287913  3.438811  3.326202  \n",
       "4    3.168760  3.867359  3.049750  2.750779  \n",
       "..        ...       ...       ...       ...  \n",
       "695 -2.593610  2.974788 -0.992141 -0.669199  \n",
       "696 -1.067273  1.911158  2.666463  0.780191  \n",
       "697  0.159113  2.839631  0.013598  0.681762  \n",
       "698 -0.538000  1.899725 -0.097139 -1.321670  \n",
       "699 -1.565722  0.025623  1.603908  1.013197  \n",
       "\n",
       "[700 rows x 18 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to pandas df\n",
    "X_data = pd.DataFrame(X_data)\n",
    "X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1.])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # normalizing values of input df between -1 and 1\n",
    "# norm_transformer = MaxAbsScaler().fit(X_train.append(X_test, ignore_index=True))\n",
    "\n",
    "# X_train_norm_values = norm_transformer.transform(X_train)\n",
    "# X_train = pd.DataFrame(X_train_norm_values, index=X_train.index, columns=X_train.columns)\n",
    "\n",
    "# X_test_norm_values = norm_transformer.transform(X_test)\n",
    "# X_test = pd.DataFrame(X_test_norm_values, index=X_test.index, columns=X_test.columns)\n",
    "\n",
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_data shape = (700, 18)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_data shape = {}\".format(np.shape(X_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_builder(\n",
    "    hidden_layers, dropout_rate, l2_regularizer, epochs, batch_size,\n",
    "):\n",
    "    return AutoEncoder(\n",
    "        hidden_neurons=hidden_layers,\n",
    "        dropout_rate=dropout_rate,\n",
    "        l2_regularizer=l2_regularizer,\n",
    "\n",
    "        hidden_activation='relu',\n",
    "        output_activation='sigmoid',\n",
    "        optimizer='adam',\n",
    "        loss=mean_squared_error,\n",
    "        \n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        \n",
    "        validation_size=0,\n",
    "        preprocessing=True\n",
    "    )\n",
    "\n",
    "\n",
    "def build_models(builder_method, *args_tests):\n",
    "    \"\"\"\n",
    "    Build models with all the combinations of the args_test passed\n",
    "    \"\"\"\n",
    "    builder_args_names = list(inspect.signature(builder_method).parameters.keys())\n",
    "    return [\n",
    "        dict(((\"model\", builder_method(*args)),) + tuple(zip(builder_args_names, args)))\n",
    "        for args in itertools.product(*args_tests)\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters definition\n",
    "input_dim = 18\n",
    "\n",
    "hidden_layers_tests = [\n",
    "    [input_dim, 8, 4, 8, input_dim],\n",
    "    [input_dim, 12, 8, 4, 8, 12, input_dim],\n",
    "    [input_dim, 10, 6, 10, input_dim],\n",
    "    [input_dim, 14, 10, 6, 10, 14, input_dim],\n",
    "    [input_dim, 16, 8, 16, input_dim],\n",
    "    [input_dim, 16, 12, 8, 12, 16, input_dim],\n",
    "]\n",
    "dropout_rate_tests = [0.2, 0.1, 0.05]\n",
    "l2_regularizer_tests = [0.1, 0.05, 0.01]\n",
    "\n",
    "# fixed\n",
    "epochs = [50]\n",
    "batch_size = [25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.2, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 8, 4, 8, 18],\n",
       "        l2_regularizer=0.1,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 8, 4, 8, 18],\n",
       "  'dropout_rate': 0.2,\n",
       "  'l2_regularizer': 0.1,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.2, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 8, 4, 8, 18],\n",
       "        l2_regularizer=0.05,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 8, 4, 8, 18],\n",
       "  'dropout_rate': 0.2,\n",
       "  'l2_regularizer': 0.05,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.2, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 8, 4, 8, 18],\n",
       "        l2_regularizer=0.01,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 8, 4, 8, 18],\n",
       "  'dropout_rate': 0.2,\n",
       "  'l2_regularizer': 0.01,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.1, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 8, 4, 8, 18],\n",
       "        l2_regularizer=0.1,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 8, 4, 8, 18],\n",
       "  'dropout_rate': 0.1,\n",
       "  'l2_regularizer': 0.1,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.1, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 8, 4, 8, 18],\n",
       "        l2_regularizer=0.05,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 8, 4, 8, 18],\n",
       "  'dropout_rate': 0.1,\n",
       "  'l2_regularizer': 0.05,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.1, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 8, 4, 8, 18],\n",
       "        l2_regularizer=0.01,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 8, 4, 8, 18],\n",
       "  'dropout_rate': 0.1,\n",
       "  'l2_regularizer': 0.01,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.05, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 8, 4, 8, 18],\n",
       "        l2_regularizer=0.1,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 8, 4, 8, 18],\n",
       "  'dropout_rate': 0.05,\n",
       "  'l2_regularizer': 0.1,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.05, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 8, 4, 8, 18],\n",
       "        l2_regularizer=0.05,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 8, 4, 8, 18],\n",
       "  'dropout_rate': 0.05,\n",
       "  'l2_regularizer': 0.05,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.05, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 8, 4, 8, 18],\n",
       "        l2_regularizer=0.01,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 8, 4, 8, 18],\n",
       "  'dropout_rate': 0.05,\n",
       "  'l2_regularizer': 0.01,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.2, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 12, 8, 4, 8, 12, 18],\n",
       "        l2_regularizer=0.1,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 12, 8, 4, 8, 12, 18],\n",
       "  'dropout_rate': 0.2,\n",
       "  'l2_regularizer': 0.1,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.2, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 12, 8, 4, 8, 12, 18],\n",
       "        l2_regularizer=0.05,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 12, 8, 4, 8, 12, 18],\n",
       "  'dropout_rate': 0.2,\n",
       "  'l2_regularizer': 0.05,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.2, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 12, 8, 4, 8, 12, 18],\n",
       "        l2_regularizer=0.01,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 12, 8, 4, 8, 12, 18],\n",
       "  'dropout_rate': 0.2,\n",
       "  'l2_regularizer': 0.01,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.1, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 12, 8, 4, 8, 12, 18],\n",
       "        l2_regularizer=0.1,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 12, 8, 4, 8, 12, 18],\n",
       "  'dropout_rate': 0.1,\n",
       "  'l2_regularizer': 0.1,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.1, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 12, 8, 4, 8, 12, 18],\n",
       "        l2_regularizer=0.05,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 12, 8, 4, 8, 12, 18],\n",
       "  'dropout_rate': 0.1,\n",
       "  'l2_regularizer': 0.05,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.1, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 12, 8, 4, 8, 12, 18],\n",
       "        l2_regularizer=0.01,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 12, 8, 4, 8, 12, 18],\n",
       "  'dropout_rate': 0.1,\n",
       "  'l2_regularizer': 0.01,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.05, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 12, 8, 4, 8, 12, 18],\n",
       "        l2_regularizer=0.1,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 12, 8, 4, 8, 12, 18],\n",
       "  'dropout_rate': 0.05,\n",
       "  'l2_regularizer': 0.1,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.05, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 12, 8, 4, 8, 12, 18],\n",
       "        l2_regularizer=0.05,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 12, 8, 4, 8, 12, 18],\n",
       "  'dropout_rate': 0.05,\n",
       "  'l2_regularizer': 0.05,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.05, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 12, 8, 4, 8, 12, 18],\n",
       "        l2_regularizer=0.01,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 12, 8, 4, 8, 12, 18],\n",
       "  'dropout_rate': 0.05,\n",
       "  'l2_regularizer': 0.01,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.2, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 10, 6, 10, 18],\n",
       "        l2_regularizer=0.1,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 10, 6, 10, 18],\n",
       "  'dropout_rate': 0.2,\n",
       "  'l2_regularizer': 0.1,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.2, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 10, 6, 10, 18],\n",
       "        l2_regularizer=0.05,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 10, 6, 10, 18],\n",
       "  'dropout_rate': 0.2,\n",
       "  'l2_regularizer': 0.05,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.2, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 10, 6, 10, 18],\n",
       "        l2_regularizer=0.01,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 10, 6, 10, 18],\n",
       "  'dropout_rate': 0.2,\n",
       "  'l2_regularizer': 0.01,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.1, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 10, 6, 10, 18],\n",
       "        l2_regularizer=0.1,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 10, 6, 10, 18],\n",
       "  'dropout_rate': 0.1,\n",
       "  'l2_regularizer': 0.1,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.1, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 10, 6, 10, 18],\n",
       "        l2_regularizer=0.05,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 10, 6, 10, 18],\n",
       "  'dropout_rate': 0.1,\n",
       "  'l2_regularizer': 0.05,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.1, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 10, 6, 10, 18],\n",
       "        l2_regularizer=0.01,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 10, 6, 10, 18],\n",
       "  'dropout_rate': 0.1,\n",
       "  'l2_regularizer': 0.01,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.05, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 10, 6, 10, 18],\n",
       "        l2_regularizer=0.1,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 10, 6, 10, 18],\n",
       "  'dropout_rate': 0.05,\n",
       "  'l2_regularizer': 0.1,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.05, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 10, 6, 10, 18],\n",
       "        l2_regularizer=0.05,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 10, 6, 10, 18],\n",
       "  'dropout_rate': 0.05,\n",
       "  'l2_regularizer': 0.05,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.05, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 10, 6, 10, 18],\n",
       "        l2_regularizer=0.01,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 10, 6, 10, 18],\n",
       "  'dropout_rate': 0.05,\n",
       "  'l2_regularizer': 0.01,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.2, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 14, 10, 6, 10, 14, 18],\n",
       "        l2_regularizer=0.1,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 14, 10, 6, 10, 14, 18],\n",
       "  'dropout_rate': 0.2,\n",
       "  'l2_regularizer': 0.1,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.2, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 14, 10, 6, 10, 14, 18],\n",
       "        l2_regularizer=0.05,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 14, 10, 6, 10, 14, 18],\n",
       "  'dropout_rate': 0.2,\n",
       "  'l2_regularizer': 0.05,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.2, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 14, 10, 6, 10, 14, 18],\n",
       "        l2_regularizer=0.01,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 14, 10, 6, 10, 14, 18],\n",
       "  'dropout_rate': 0.2,\n",
       "  'l2_regularizer': 0.01,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.1, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 14, 10, 6, 10, 14, 18],\n",
       "        l2_regularizer=0.1,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 14, 10, 6, 10, 14, 18],\n",
       "  'dropout_rate': 0.1,\n",
       "  'l2_regularizer': 0.1,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.1, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 14, 10, 6, 10, 14, 18],\n",
       "        l2_regularizer=0.05,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 14, 10, 6, 10, 14, 18],\n",
       "  'dropout_rate': 0.1,\n",
       "  'l2_regularizer': 0.05,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.1, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 14, 10, 6, 10, 14, 18],\n",
       "        l2_regularizer=0.01,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 14, 10, 6, 10, 14, 18],\n",
       "  'dropout_rate': 0.1,\n",
       "  'l2_regularizer': 0.01,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.05, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 14, 10, 6, 10, 14, 18],\n",
       "        l2_regularizer=0.1,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 14, 10, 6, 10, 14, 18],\n",
       "  'dropout_rate': 0.05,\n",
       "  'l2_regularizer': 0.1,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.05, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 14, 10, 6, 10, 14, 18],\n",
       "        l2_regularizer=0.05,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 14, 10, 6, 10, 14, 18],\n",
       "  'dropout_rate': 0.05,\n",
       "  'l2_regularizer': 0.05,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.05, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 14, 10, 6, 10, 14, 18],\n",
       "        l2_regularizer=0.01,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 14, 10, 6, 10, 14, 18],\n",
       "  'dropout_rate': 0.05,\n",
       "  'l2_regularizer': 0.01,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.2, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 16, 8, 16, 18],\n",
       "        l2_regularizer=0.1,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 16, 8, 16, 18],\n",
       "  'dropout_rate': 0.2,\n",
       "  'l2_regularizer': 0.1,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.2, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 16, 8, 16, 18],\n",
       "        l2_regularizer=0.05,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 16, 8, 16, 18],\n",
       "  'dropout_rate': 0.2,\n",
       "  'l2_regularizer': 0.05,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.2, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 16, 8, 16, 18],\n",
       "        l2_regularizer=0.01,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 16, 8, 16, 18],\n",
       "  'dropout_rate': 0.2,\n",
       "  'l2_regularizer': 0.01,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.1, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 16, 8, 16, 18],\n",
       "        l2_regularizer=0.1,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 16, 8, 16, 18],\n",
       "  'dropout_rate': 0.1,\n",
       "  'l2_regularizer': 0.1,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.1, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 16, 8, 16, 18],\n",
       "        l2_regularizer=0.05,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 16, 8, 16, 18],\n",
       "  'dropout_rate': 0.1,\n",
       "  'l2_regularizer': 0.05,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.1, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 16, 8, 16, 18],\n",
       "        l2_regularizer=0.01,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 16, 8, 16, 18],\n",
       "  'dropout_rate': 0.1,\n",
       "  'l2_regularizer': 0.01,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.05, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 16, 8, 16, 18],\n",
       "        l2_regularizer=0.1,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 16, 8, 16, 18],\n",
       "  'dropout_rate': 0.05,\n",
       "  'l2_regularizer': 0.1,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.05, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 16, 8, 16, 18],\n",
       "        l2_regularizer=0.05,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 16, 8, 16, 18],\n",
       "  'dropout_rate': 0.05,\n",
       "  'l2_regularizer': 0.05,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.05, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 16, 8, 16, 18],\n",
       "        l2_regularizer=0.01,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 16, 8, 16, 18],\n",
       "  'dropout_rate': 0.05,\n",
       "  'l2_regularizer': 0.01,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.2, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 16, 12, 8, 12, 16, 18],\n",
       "        l2_regularizer=0.1,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 16, 12, 8, 12, 16, 18],\n",
       "  'dropout_rate': 0.2,\n",
       "  'l2_regularizer': 0.1,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.2, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 16, 12, 8, 12, 16, 18],\n",
       "        l2_regularizer=0.05,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 16, 12, 8, 12, 16, 18],\n",
       "  'dropout_rate': 0.2,\n",
       "  'l2_regularizer': 0.05,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.2, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 16, 12, 8, 12, 16, 18],\n",
       "        l2_regularizer=0.01,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 16, 12, 8, 12, 16, 18],\n",
       "  'dropout_rate': 0.2,\n",
       "  'l2_regularizer': 0.01,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.1, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 16, 12, 8, 12, 16, 18],\n",
       "        l2_regularizer=0.1,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 16, 12, 8, 12, 16, 18],\n",
       "  'dropout_rate': 0.1,\n",
       "  'l2_regularizer': 0.1,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.1, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 16, 12, 8, 12, 16, 18],\n",
       "        l2_regularizer=0.05,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 16, 12, 8, 12, 16, 18],\n",
       "  'dropout_rate': 0.1,\n",
       "  'l2_regularizer': 0.05,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.1, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 16, 12, 8, 12, 16, 18],\n",
       "        l2_regularizer=0.01,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 16, 12, 8, 12, 16, 18],\n",
       "  'dropout_rate': 0.1,\n",
       "  'l2_regularizer': 0.01,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.05, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 16, 12, 8, 12, 16, 18],\n",
       "        l2_regularizer=0.1,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 16, 12, 8, 12, 16, 18],\n",
       "  'dropout_rate': 0.05,\n",
       "  'l2_regularizer': 0.1,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.05, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 16, 12, 8, 12, 16, 18],\n",
       "        l2_regularizer=0.05,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 16, 12, 8, 12, 16, 18],\n",
       "  'dropout_rate': 0.05,\n",
       "  'l2_regularizer': 0.05,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25},\n",
       " {'model': AutoEncoder(batch_size=25, contamination=0.1, dropout_rate=0.05, epochs=50,\n",
       "        hidden_activation='relu', hidden_neurons=[18, 16, 12, 8, 12, 16, 18],\n",
       "        l2_regularizer=0.01,\n",
       "        loss=<function mean_squared_error at 0x7ff32a0c2830>,\n",
       "        optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "        random_state=None, validation_size=0, verbose=1),\n",
       "  'hidden_layers': [18, 16, 12, 8, 12, 16, 18],\n",
       "  'dropout_rate': 0.05,\n",
       "  'l2_regularizer': 0.01,\n",
       "  'epochs': 50,\n",
       "  'batch_size': 25}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building models with all the combinations of the test parameters\n",
    "models = build_models(\n",
    "    autoencoder_builder,\n",
    "    hidden_layers_tests,\n",
    "    dropout_rate_tests,\n",
    "    l2_regularizer_tests,\n",
    "    epochs,\n",
    "    batch_size,\n",
    ")\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models K-fold evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 152       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 18)                162       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 18)                342       \n",
      "=================================================================\n",
      "Total params: 1,758\n",
      "Trainable params: 1,758\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "560/560 [==============================] - 0s 597us/step - loss: 46.7451\n",
      "Epoch 2/50\n",
      "560/560 [==============================] - 0s 66us/step - loss: 33.8230\n",
      "Epoch 3/50\n",
      "560/560 [==============================] - 0s 69us/step - loss: 26.6879\n",
      "Epoch 4/50\n",
      "560/560 [==============================] - 0s 71us/step - loss: 22.7508\n",
      "Epoch 5/50\n",
      "560/560 [==============================] - 0s 63us/step - loss: 19.3678\n",
      "Epoch 6/50\n",
      "560/560 [==============================] - 0s 63us/step - loss: 16.9232\n",
      "Epoch 7/50\n",
      "560/560 [==============================] - 0s 64us/step - loss: 14.7534\n",
      "Epoch 8/50\n",
      "560/560 [==============================] - 0s 63us/step - loss: 13.1531\n",
      "Epoch 9/50\n",
      "560/560 [==============================] - 0s 61us/step - loss: 11.7854\n",
      "Epoch 10/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 10.7460\n",
      "Epoch 11/50\n",
      "560/560 [==============================] - 0s 60us/step - loss: 9.7563\n",
      "Epoch 12/50\n",
      "560/560 [==============================] - 0s 60us/step - loss: 9.0572\n",
      "Epoch 13/50\n",
      "560/560 [==============================] - 0s 60us/step - loss: 8.4241\n",
      "Epoch 14/50\n",
      "560/560 [==============================] - 0s 57us/step - loss: 7.8525\n",
      "Epoch 15/50\n",
      "560/560 [==============================] - 0s 58us/step - loss: 7.3153\n",
      "Epoch 16/50\n",
      "560/560 [==============================] - 0s 73us/step - loss: 6.9379\n",
      "Epoch 17/50\n",
      "560/560 [==============================] - 0s 63us/step - loss: 6.6349\n",
      "Epoch 18/50\n",
      "560/560 [==============================] - 0s 72us/step - loss: 6.2689\n",
      "Epoch 19/50\n",
      "560/560 [==============================] - 0s 68us/step - loss: 5.9481\n",
      "Epoch 20/50\n",
      "560/560 [==============================] - 0s 62us/step - loss: 5.6693\n",
      "Epoch 21/50\n",
      "560/560 [==============================] - 0s 74us/step - loss: 5.4360\n",
      "Epoch 22/50\n",
      "560/560 [==============================] - 0s 69us/step - loss: 5.2388\n",
      "Epoch 23/50\n",
      "560/560 [==============================] - 0s 65us/step - loss: 4.9760\n",
      "Epoch 24/50\n",
      "560/560 [==============================] - 0s 67us/step - loss: 4.8187\n",
      "Epoch 25/50\n",
      "560/560 [==============================] - 0s 67us/step - loss: 4.6586\n",
      "Epoch 26/50\n",
      "560/560 [==============================] - 0s 75us/step - loss: 4.4867\n",
      "Epoch 27/50\n",
      "560/560 [==============================] - 0s 65us/step - loss: 4.3545\n",
      "Epoch 28/50\n",
      "560/560 [==============================] - 0s 67us/step - loss: 4.1953\n",
      "Epoch 29/50\n",
      "560/560 [==============================] - 0s 72us/step - loss: 4.0896\n",
      "Epoch 30/50\n",
      "560/560 [==============================] - 0s 70us/step - loss: 3.9486\n",
      "Epoch 31/50\n",
      "560/560 [==============================] - 0s 71us/step - loss: 3.8465\n",
      "Epoch 32/50\n",
      "560/560 [==============================] - 0s 73us/step - loss: 3.7443\n",
      "Epoch 33/50\n",
      "560/560 [==============================] - 0s 83us/step - loss: 3.6481\n",
      "Epoch 34/50\n",
      "560/560 [==============================] - 0s 88us/step - loss: 3.5680\n",
      "Epoch 35/50\n",
      "560/560 [==============================] - 0s 87us/step - loss: 3.4504\n",
      "Epoch 36/50\n",
      "560/560 [==============================] - 0s 71us/step - loss: 3.3724\n",
      "Epoch 37/50\n",
      "560/560 [==============================] - 0s 63us/step - loss: 3.3313\n",
      "Epoch 38/50\n",
      "560/560 [==============================] - 0s 74us/step - loss: 3.2400\n",
      "Epoch 39/50\n",
      "560/560 [==============================] - 0s 68us/step - loss: 3.1800\n",
      "Epoch 40/50\n",
      "560/560 [==============================] - 0s 77us/step - loss: 3.1237\n",
      "Epoch 41/50\n",
      "560/560 [==============================] - 0s 71us/step - loss: 3.0601\n",
      "Epoch 42/50\n",
      "560/560 [==============================] - 0s 75us/step - loss: 3.0169\n",
      "Epoch 43/50\n",
      "560/560 [==============================] - 0s 65us/step - loss: 2.9346\n",
      "Epoch 44/50\n",
      "560/560 [==============================] - 0s 73us/step - loss: 2.8827\n",
      "Epoch 45/50\n",
      "560/560 [==============================] - 0s 65us/step - loss: 2.8868\n",
      "Epoch 46/50\n",
      "560/560 [==============================] - 0s 67us/step - loss: 2.8006\n",
      "Epoch 47/50\n",
      "560/560 [==============================] - 0s 64us/step - loss: 2.7531\n",
      "Epoch 48/50\n",
      "560/560 [==============================] - 0s 64us/step - loss: 2.7194\n",
      "Epoch 49/50\n",
      "560/560 [==============================] - 0s 71us/step - loss: 2.7029\n",
      "Epoch 50/50\n",
      "560/560 [==============================] - 0s 66us/step - loss: 2.6441\n",
      "val_loss =  2.914985602242606\n",
      "Fold: 1\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 8)                 152       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 18)                162       \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 18)                342       \n",
      "=================================================================\n",
      "Total params: 1,758\n",
      "Trainable params: 1,758\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "560/560 [==============================] - 0s 622us/step - loss: 38.2998\n",
      "Epoch 2/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 27.9510\n",
      "Epoch 3/50\n",
      "560/560 [==============================] - 0s 67us/step - loss: 22.9769\n",
      "Epoch 4/50\n",
      "560/560 [==============================] - 0s 65us/step - loss: 19.3618\n",
      "Epoch 5/50\n",
      "560/560 [==============================] - 0s 60us/step - loss: 16.8368\n",
      "Epoch 6/50\n",
      "560/560 [==============================] - 0s 57us/step - loss: 14.7362\n",
      "Epoch 7/50\n",
      "560/560 [==============================] - 0s 57us/step - loss: 13.1808\n",
      "Epoch 8/50\n",
      "560/560 [==============================] - 0s 58us/step - loss: 11.7929\n",
      "Epoch 9/50\n",
      "560/560 [==============================] - 0s 61us/step - loss: 10.7234\n",
      "Epoch 10/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 9.8055\n",
      "Epoch 11/50\n",
      "560/560 [==============================] - 0s 57us/step - loss: 8.9714\n",
      "Epoch 12/50\n",
      "560/560 [==============================] - 0s 58us/step - loss: 8.3420\n",
      "Epoch 13/50\n",
      "560/560 [==============================] - 0s 57us/step - loss: 7.7639\n",
      "Epoch 14/50\n",
      "560/560 [==============================] - 0s 54us/step - loss: 7.2324\n",
      "Epoch 15/50\n",
      "560/560 [==============================] - 0s 54us/step - loss: 6.8395\n",
      "Epoch 16/50\n",
      "560/560 [==============================] - 0s 61us/step - loss: 6.4123\n",
      "Epoch 17/50\n",
      "560/560 [==============================] - 0s 58us/step - loss: 6.0717\n",
      "Epoch 18/50\n",
      "560/560 [==============================] - 0s 58us/step - loss: 5.7918\n",
      "Epoch 19/50\n",
      "560/560 [==============================] - 0s 60us/step - loss: 5.4949\n",
      "Epoch 20/50\n",
      "560/560 [==============================] - 0s 60us/step - loss: 5.2795\n",
      "Epoch 21/50\n",
      "560/560 [==============================] - 0s 58us/step - loss: 5.0340\n",
      "Epoch 22/50\n",
      "560/560 [==============================] - 0s 61us/step - loss: 4.8532\n",
      "Epoch 23/50\n",
      "560/560 [==============================] - 0s 58us/step - loss: 4.6479\n",
      "Epoch 24/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 4.4757\n",
      "Epoch 25/50\n",
      "560/560 [==============================] - 0s 61us/step - loss: 4.3336\n",
      "Epoch 26/50\n",
      "560/560 [==============================] - 0s 58us/step - loss: 4.2119\n",
      "Epoch 27/50\n",
      "560/560 [==============================] - 0s 58us/step - loss: 4.0320\n",
      "Epoch 28/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 3.9226\n",
      "Epoch 29/50\n",
      "560/560 [==============================] - 0s 56us/step - loss: 3.8247\n",
      "Epoch 30/50\n",
      "560/560 [==============================] - 0s 57us/step - loss: 3.7240\n",
      "Epoch 31/50\n",
      "560/560 [==============================] - 0s 63us/step - loss: 3.6254\n",
      "Epoch 32/50\n",
      "560/560 [==============================] - 0s 58us/step - loss: 3.5156\n",
      "Epoch 33/50\n",
      "560/560 [==============================] - 0s 58us/step - loss: 3.4421\n",
      "Epoch 34/50\n",
      "560/560 [==============================] - 0s 61us/step - loss: 3.3708\n",
      "Epoch 35/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 3.3132\n",
      "Epoch 36/50\n",
      "560/560 [==============================] - 0s 60us/step - loss: 3.2063\n",
      "Epoch 37/50\n",
      "560/560 [==============================] - 0s 58us/step - loss: 3.1419\n",
      "Epoch 38/50\n",
      "560/560 [==============================] - 0s 56us/step - loss: 3.1123\n",
      "Epoch 39/50\n",
      "560/560 [==============================] - 0s 55us/step - loss: 3.0080\n",
      "Epoch 40/50\n",
      "560/560 [==============================] - 0s 60us/step - loss: 2.9826\n",
      "Epoch 41/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 2.9088\n",
      "Epoch 42/50\n",
      "560/560 [==============================] - 0s 58us/step - loss: 2.8819\n",
      "Epoch 43/50\n",
      "560/560 [==============================] - 0s 62us/step - loss: 2.8372\n",
      "Epoch 44/50\n",
      "560/560 [==============================] - 0s 57us/step - loss: 2.7836\n",
      "Epoch 45/50\n",
      "560/560 [==============================] - 0s 56us/step - loss: 2.7662\n",
      "Epoch 46/50\n",
      "560/560 [==============================] - 0s 61us/step - loss: 2.7139\n",
      "Epoch 47/50\n",
      "560/560 [==============================] - 0s 54us/step - loss: 2.6708\n",
      "Epoch 48/50\n",
      "560/560 [==============================] - 0s 54us/step - loss: 2.6232\n",
      "Epoch 49/50\n",
      "560/560 [==============================] - 0s 57us/step - loss: 2.5974\n",
      "Epoch 50/50\n",
      "560/560 [==============================] - 0s 51us/step - loss: 2.5431\n",
      "val_loss =  2.6594114303588867\n",
      "Fold: 2\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_17 (Dense)             (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 8)                 152       \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 18)                162       \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 18)                342       \n",
      "=================================================================\n",
      "Total params: 1,758\n",
      "Trainable params: 1,758\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "560/560 [==============================] - 0s 613us/step - loss: 42.9321\n",
      "Epoch 2/50\n",
      "560/560 [==============================] - 0s 66us/step - loss: 30.1896\n",
      "Epoch 3/50\n",
      "560/560 [==============================] - 0s 68us/step - loss: 24.2107\n",
      "Epoch 4/50\n",
      "560/560 [==============================] - 0s 65us/step - loss: 21.0065\n",
      "Epoch 5/50\n",
      "560/560 [==============================] - 0s 60us/step - loss: 17.8689\n",
      "Epoch 6/50\n",
      "560/560 [==============================] - 0s 60us/step - loss: 15.7791\n",
      "Epoch 7/50\n",
      "560/560 [==============================] - 0s 61us/step - loss: 13.8616\n",
      "Epoch 8/50\n",
      "560/560 [==============================] - 0s 58us/step - loss: 12.2799\n",
      "Epoch 9/50\n",
      "560/560 [==============================] - 0s 61us/step - loss: 11.2599\n",
      "Epoch 10/50\n",
      "560/560 [==============================] - 0s 60us/step - loss: 10.2107\n",
      "Epoch 11/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 9.4828\n",
      "Epoch 12/50\n",
      "560/560 [==============================] - 0s 57us/step - loss: 8.6963\n",
      "Epoch 13/50\n",
      "560/560 [==============================] - 0s 57us/step - loss: 8.1747\n",
      "Epoch 14/50\n",
      "560/560 [==============================] - 0s 58us/step - loss: 7.7252\n",
      "Epoch 15/50\n",
      "560/560 [==============================] - 0s 55us/step - loss: 7.2827\n",
      "Epoch 16/50\n",
      "560/560 [==============================] - 0s 58us/step - loss: 6.8654\n",
      "Epoch 17/50\n",
      "560/560 [==============================] - 0s 57us/step - loss: 6.5032\n",
      "Epoch 18/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 6.1785\n",
      "Epoch 19/50\n",
      "560/560 [==============================] - 0s 70us/step - loss: 5.8761\n",
      "Epoch 20/50\n",
      "560/560 [==============================] - 0s 68us/step - loss: 5.6192\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560/560 [==============================] - 0s 63us/step - loss: 5.3335\n",
      "Epoch 22/50\n",
      "560/560 [==============================] - 0s 62us/step - loss: 5.1398\n",
      "Epoch 23/50\n",
      "560/560 [==============================] - 0s 54us/step - loss: 4.9594\n",
      "Epoch 24/50\n",
      "560/560 [==============================] - 0s 54us/step - loss: 4.7675\n",
      "Epoch 25/50\n",
      "560/560 [==============================] - 0s 55us/step - loss: 4.6218\n",
      "Epoch 26/50\n",
      "560/560 [==============================] - 0s 64us/step - loss: 4.4512\n",
      "Epoch 27/50\n",
      "560/560 [==============================] - 0s 53us/step - loss: 4.2735\n",
      "Epoch 28/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 4.1913\n",
      "Epoch 29/50\n",
      "560/560 [==============================] - 0s 54us/step - loss: 4.0544\n",
      "Epoch 30/50\n",
      "560/560 [==============================] - 0s 55us/step - loss: 3.9162\n",
      "Epoch 31/50\n",
      "560/560 [==============================] - 0s 63us/step - loss: 3.8324\n",
      "Epoch 32/50\n",
      "560/560 [==============================] - 0s 54us/step - loss: 3.7236\n",
      "Epoch 33/50\n",
      "560/560 [==============================] - 0s 60us/step - loss: 3.6390\n",
      "Epoch 34/50\n",
      "560/560 [==============================] - 0s 63us/step - loss: 3.5411\n",
      "Epoch 35/50\n",
      "560/560 [==============================] - 0s 55us/step - loss: 3.4584\n",
      "Epoch 36/50\n",
      "560/560 [==============================] - 0s 64us/step - loss: 3.3929\n",
      "Epoch 37/50\n",
      "560/560 [==============================] - 0s 54us/step - loss: 3.3043\n",
      "Epoch 38/50\n",
      "560/560 [==============================] - 0s 54us/step - loss: 3.2519\n",
      "Epoch 39/50\n",
      "560/560 [==============================] - 0s 62us/step - loss: 3.1802\n",
      "Epoch 40/50\n",
      "560/560 [==============================] - 0s 57us/step - loss: 3.1118\n",
      "Epoch 41/50\n",
      "560/560 [==============================] - 0s 60us/step - loss: 3.0453\n",
      "Epoch 42/50\n",
      "560/560 [==============================] - 0s 57us/step - loss: 2.9970\n",
      "Epoch 43/50\n",
      "560/560 [==============================] - 0s 62us/step - loss: 2.9592\n",
      "Epoch 44/50\n",
      "560/560 [==============================] - 0s 67us/step - loss: 2.9093\n",
      "Epoch 45/50\n",
      "560/560 [==============================] - 0s 54us/step - loss: 2.8675\n",
      "Epoch 46/50\n",
      "560/560 [==============================] - 0s 62us/step - loss: 2.8157\n",
      "Epoch 47/50\n",
      "560/560 [==============================] - 0s 62us/step - loss: 2.7829\n",
      "Epoch 48/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 2.7352\n",
      "Epoch 49/50\n",
      "560/560 [==============================] - 0s 62us/step - loss: 2.6747\n",
      "Epoch 50/50\n",
      "560/560 [==============================] - 0s 61us/step - loss: 2.6525\n",
      "val_loss =  2.934698976789202\n",
      "Fold: 3\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 8)                 152       \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 18)                162       \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 18)                342       \n",
      "=================================================================\n",
      "Total params: 1,758\n",
      "Trainable params: 1,758\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "560/560 [==============================] - 0s 590us/step - loss: 64.7242\n",
      "Epoch 2/50\n",
      "560/560 [==============================] - 0s 63us/step - loss: 45.9512\n",
      "Epoch 3/50\n",
      "560/560 [==============================] - 0s 67us/step - loss: 36.3118\n",
      "Epoch 4/50\n",
      "560/560 [==============================] - 0s 76us/step - loss: 29.8163\n",
      "Epoch 5/50\n",
      "560/560 [==============================] - 0s 63us/step - loss: 25.0767\n",
      "Epoch 6/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 20.9092\n",
      "Epoch 7/50\n",
      "560/560 [==============================] - 0s 60us/step - loss: 18.2119\n",
      "Epoch 8/50\n",
      "560/560 [==============================] - 0s 57us/step - loss: 16.0645\n",
      "Epoch 9/50\n",
      "560/560 [==============================] - 0s 56us/step - loss: 14.1109\n",
      "Epoch 10/50\n",
      "560/560 [==============================] - 0s 63us/step - loss: 12.6646\n",
      "Epoch 11/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 11.5013\n",
      "Epoch 12/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 10.4999\n",
      "Epoch 13/50\n",
      "560/560 [==============================] - 0s 64us/step - loss: 9.7860\n",
      "Epoch 14/50\n",
      "560/560 [==============================] - 0s 61us/step - loss: 8.9880\n",
      "Epoch 15/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 8.4839\n",
      "Epoch 16/50\n",
      "560/560 [==============================] - 0s 67us/step - loss: 7.9115\n",
      "Epoch 17/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 7.3817\n",
      "Epoch 18/50\n",
      "560/560 [==============================] - 0s 62us/step - loss: 7.1175\n",
      "Epoch 19/50\n",
      "560/560 [==============================] - 0s 61us/step - loss: 6.7281\n",
      "Epoch 20/50\n",
      "560/560 [==============================] - 0s 60us/step - loss: 6.4363\n",
      "Epoch 21/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 6.1244\n",
      "Epoch 22/50\n",
      "560/560 [==============================] - 0s 58us/step - loss: 5.9313\n",
      "Epoch 23/50\n",
      "560/560 [==============================] - 0s 57us/step - loss: 5.6821\n",
      "Epoch 24/50\n",
      "560/560 [==============================] - 0s 57us/step - loss: 5.4246\n",
      "Epoch 25/50\n",
      "560/560 [==============================] - 0s 57us/step - loss: 5.2342\n",
      "Epoch 26/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 5.0694\n",
      "Epoch 27/50\n",
      "560/560 [==============================] - 0s 58us/step - loss: 4.8887\n",
      "Epoch 28/50\n",
      "560/560 [==============================] - 0s 60us/step - loss: 4.6773\n",
      "Epoch 29/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 4.5359\n",
      "Epoch 30/50\n",
      "560/560 [==============================] - 0s 60us/step - loss: 4.4536\n",
      "Epoch 31/50\n",
      "560/560 [==============================] - 0s 58us/step - loss: 4.3043\n",
      "Epoch 32/50\n",
      "560/560 [==============================] - 0s 57us/step - loss: 4.1648\n",
      "Epoch 33/50\n",
      "560/560 [==============================] - 0s 57us/step - loss: 4.0638\n",
      "Epoch 34/50\n",
      "560/560 [==============================] - 0s 61us/step - loss: 3.9565\n",
      "Epoch 35/50\n",
      "560/560 [==============================] - 0s 57us/step - loss: 3.8366\n",
      "Epoch 36/50\n",
      "560/560 [==============================] - 0s 62us/step - loss: 3.7435\n",
      "Epoch 37/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 3.6658\n",
      "Epoch 38/50\n",
      "560/560 [==============================] - 0s 58us/step - loss: 3.5769\n",
      "Epoch 39/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 3.5039\n",
      "Epoch 40/50\n",
      "560/560 [==============================] - 0s 62us/step - loss: 3.4203\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560/560 [==============================] - 0s 59us/step - loss: 3.3755\n",
      "Epoch 42/50\n",
      "560/560 [==============================] - 0s 63us/step - loss: 3.2888\n",
      "Epoch 43/50\n",
      "560/560 [==============================] - 0s 58us/step - loss: 3.1984\n",
      "Epoch 44/50\n",
      "560/560 [==============================] - 0s 55us/step - loss: 3.1745\n",
      "Epoch 45/50\n",
      "560/560 [==============================] - 0s 57us/step - loss: 3.1038\n",
      "Epoch 46/50\n",
      "560/560 [==============================] - 0s 56us/step - loss: 3.0260\n",
      "Epoch 47/50\n",
      "560/560 [==============================] - 0s 57us/step - loss: 3.0003\n",
      "Epoch 48/50\n",
      "560/560 [==============================] - 0s 57us/step - loss: 2.9516\n",
      "Epoch 49/50\n",
      "560/560 [==============================] - 0s 58us/step - loss: 2.9112\n",
      "Epoch 50/50\n",
      "560/560 [==============================] - 0s 57us/step - loss: 2.8670\n",
      "val_loss =  3.5224688393729076\n",
      "Fold: 4\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_33 (Dense)             (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 8)                 152       \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 18)                162       \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 18)                342       \n",
      "=================================================================\n",
      "Total params: 1,758\n",
      "Trainable params: 1,758\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "560/560 [==============================] - 0s 647us/step - loss: 55.5769\n",
      "Epoch 2/50\n",
      "560/560 [==============================] - 0s 62us/step - loss: 39.1579\n",
      "Epoch 3/50\n",
      "560/560 [==============================] - 0s 61us/step - loss: 31.1505\n",
      "Epoch 4/50\n",
      "560/560 [==============================] - 0s 69us/step - loss: 25.1515\n",
      "Epoch 5/50\n",
      "560/560 [==============================] - 0s 61us/step - loss: 21.7439\n",
      "Epoch 6/50\n",
      "560/560 [==============================] - 0s 58us/step - loss: 18.8569\n",
      "Epoch 7/50\n",
      "560/560 [==============================] - 0s 57us/step - loss: 16.6458\n",
      "Epoch 8/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 14.5369\n",
      "Epoch 9/50\n",
      "560/560 [==============================] - 0s 58us/step - loss: 13.0461\n",
      "Epoch 10/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 11.6470\n",
      "Epoch 11/50\n",
      "560/560 [==============================] - 0s 56us/step - loss: 10.7368\n",
      "Epoch 12/50\n",
      "560/560 [==============================] - 0s 56us/step - loss: 9.9275\n",
      "Epoch 13/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 9.1348\n",
      "Epoch 14/50\n",
      "560/560 [==============================] - 0s 60us/step - loss: 8.5328\n",
      "Epoch 15/50\n",
      "560/560 [==============================] - 0s 69us/step - loss: 8.0326\n",
      "Epoch 16/50\n",
      "560/560 [==============================] - 0s 60us/step - loss: 7.5625\n",
      "Epoch 17/50\n",
      "560/560 [==============================] - 0s 60us/step - loss: 7.0874\n",
      "Epoch 18/50\n",
      "560/560 [==============================] - 0s 56us/step - loss: 6.7711\n",
      "Epoch 19/50\n",
      "560/560 [==============================] - 0s 58us/step - loss: 6.4939\n",
      "Epoch 20/50\n",
      "560/560 [==============================] - 0s 57us/step - loss: 6.1716\n",
      "Epoch 21/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 5.8886\n",
      "Epoch 22/50\n",
      "560/560 [==============================] - 0s 64us/step - loss: 5.5827\n",
      "Epoch 23/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 5.4502\n",
      "Epoch 24/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 5.2731\n",
      "Epoch 25/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 5.0558\n",
      "Epoch 26/50\n",
      "560/560 [==============================] - 0s 62us/step - loss: 4.8979\n",
      "Epoch 27/50\n",
      "560/560 [==============================] - 0s 61us/step - loss: 4.7028\n",
      "Epoch 28/50\n",
      "560/560 [==============================] - 0s 56us/step - loss: 4.5458\n",
      "Epoch 29/50\n",
      "560/560 [==============================] - 0s 53us/step - loss: 4.4031\n",
      "Epoch 30/50\n",
      "560/560 [==============================] - 0s 58us/step - loss: 4.3109\n",
      "Epoch 31/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 4.1667\n",
      "Epoch 32/50\n",
      "560/560 [==============================] - 0s 49us/step - loss: 4.0736\n",
      "Epoch 33/50\n",
      "560/560 [==============================] - 0s 58us/step - loss: 3.9799\n",
      "Epoch 34/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 3.8600\n",
      "Epoch 35/50\n",
      "560/560 [==============================] - 0s 57us/step - loss: 3.7768\n",
      "Epoch 36/50\n",
      "560/560 [==============================] - 0s 59us/step - loss: 3.7022\n",
      "Epoch 37/50\n",
      "560/560 [==============================] - 0s 61us/step - loss: 3.6317\n",
      "Epoch 38/50\n",
      "560/560 [==============================] - 0s 60us/step - loss: 3.5581\n",
      "Epoch 39/50\n",
      "560/560 [==============================] - 0s 63us/step - loss: 3.4292\n",
      "Epoch 40/50\n",
      "560/560 [==============================] - 0s 76us/step - loss: 3.3920\n",
      "Epoch 41/50\n",
      "560/560 [==============================] - 0s 75us/step - loss: 3.3192\n",
      "Epoch 42/50\n",
      "560/560 [==============================] - 0s 72us/step - loss: 3.2481\n",
      "Epoch 43/50\n",
      "560/560 [==============================] - 0s 67us/step - loss: 3.1572\n",
      "Epoch 44/50\n",
      "560/560 [==============================] - 0s 60us/step - loss: 3.1603\n",
      "Epoch 45/50\n",
      "560/560 [==============================] - 0s 66us/step - loss: 3.0991\n",
      "Epoch 46/50\n",
      "560/560 [==============================] - 0s 60us/step - loss: 3.0431\n",
      "Epoch 47/50\n",
      "560/560 [==============================] - 0s 60us/step - loss: 2.9944\n",
      "Epoch 48/50\n",
      "560/560 [==============================] - 0s 58us/step - loss: 2.9170\n",
      "Epoch 49/50\n",
      "560/560 [==============================] - 0s 60us/step - loss: 2.9284\n",
      "Epoch 50/50\n",
      "560/560 [==============================] - 0s 64us/step - loss: 2.8325\n",
      "val_loss =  3.289533942086356\n",
      "CPU times: user 28.1 s, sys: 1.45 s, total: 29.6 s\n",
      "Wall time: 20.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for model in models[]:\n",
    "    k_fold_val_losses = []\n",
    "    temp_model = autoencoder_builder(\n",
    "        model[\"hidden_layers\"],\n",
    "        model[\"dropout_rate\"],\n",
    "        model[\"l2_regularizer\"],\n",
    "        model[\"epochs\"],\n",
    "        model[\"batch_size\"],\n",
    "    )\n",
    "    \n",
    "    for i, indexes in enumerate(kfold.split(X_data, np.zeros(np.shape(X_data)[0]))):\n",
    "        train_index, test_index = indexes\n",
    "        print(\"Fold:\", i)\n",
    "\n",
    "        temp_model.fit(X_data.iloc[train_index])\n",
    "\n",
    "        val_loss = temp_model.model_.evaluate(\n",
    "            temp_model.scaler_.transform(X_data.iloc[test_index]),\n",
    "            temp_model.scaler_.transform(X_data.iloc[test_index]),\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        print(\"val_loss = \", val_loss)\n",
    "        k_fold_val_losses.append(val_loss)\n",
    "    \n",
    "    model[\"k_fold_evaluation_loss\"] = statistics.mean(k_fold_val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.22654026 2.99080783 3.79284053 2.09676233 2.25505867 2.63120499\n",
      "  2.7417184  2.1115748  3.13047573 2.63056753 2.08208824 2.44133275\n",
      "  3.68813115 2.73109771 2.89959418 3.55404183 3.17969014 2.34477686]]\n",
      "\n",
      "[[ 0.40482614  0.21759861  0.89099185 -0.5057645  -0.42358449 -0.07648761\n",
      "   0.01039764 -0.50844703  0.33959281 -0.02704015 -0.63818407 -0.21748741\n",
      "   0.86589665 -0.04365958  0.16333738  0.71720844  0.38898887 -0.31806403]]\n",
      "\n",
      "[[0.12418073 0.12204647 0.11525089 0.13150167 0.11556122 0.12325948\n",
      "  0.10748079 0.11822954 0.11672813 0.12141585 0.10796648 0.13041407\n",
      "  0.11734381 0.12280864 0.12171713 0.12984389 0.12582947 0.12142564]]\n",
      "\n",
      "0.27736037969589233\n",
      "\n",
      "[1.94297717]\n"
     ]
    }
   ],
   "source": [
    "# b = X_data.iloc[[10]]\n",
    "# a = pd.DataFrame(temp_model.scaler_.transform(b))\n",
    "# print(b.values)\n",
    "# print()\n",
    "# print(a.values)\n",
    "# print()\n",
    "# print(temp_model.model_.predict(a))\n",
    "# print()\n",
    "# print(temp_model.model_.evaluate(a, a, verbose=False))\n",
    "# print()\n",
    "# print(temp_model.decision_function(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Models in K-fold evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting by test dataset accuracy, from higher to lower\n",
    "sorted_models = sorted(models, key=lambda d: d['k_fold_evaluation_loss'], reverse=True)\n",
    "for i, model in enumerate(sorted_models):\n",
    "    print(\"TOP {}:\".format(i + 1))\n",
    "    print(\"    hidden_laye222rs: {}\".format(model[\"hidden_layers\"]))\n",
    "    print(\"    l2_value: {}\".format(model[\"l2_value\"]))\n",
    "    print(\"    momentum_beta: {}\".format(model[\"momentum_beta\"]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retraining TOP 5 models - Building Essemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 1 history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 2 history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 3 history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 4 history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
