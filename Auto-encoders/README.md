# Auto-encoders Project
> Author: Rafael Leiniö
> Neural Networks Postgraduate Class - National Institute of Space Research (INPE) - Prof. Dr. Marcos Gonçalves Quiles

# Introduction
Self Organizing Maps (SOM) or Kohonen networks, was developed by Teuvo Kohonen in 1982, being considered relatively simple and with the ability to reduce dimensionality of the original features, organizing complex data into clusters according to their relationships. This method only requests input parameters and is ideal for problems where patterns are unknown or undetermined.

![](https://cdn.educba.com/academy/wp-content/uploads/2019/09/Autoencoders-3.png)

The objective of this project is to test SOM, testing models with different sigmas and learning rates. Three classification datasets were tested.

# Experiments
There is 3 notebooks in this folder with the implementation of the experiments, one for each dataset. The Auto-encoder network specification parameters tested in the notebooks are the following:

Fixed parameters (were the same for all tests):

* Input Layer: specific of each dataset
* Output Layer: specific of each dataset (same as input)
* Activation Function: ReLU in hidden layers and sigmoid in output Layer
* Loss function: binary cross entropy (as requested in the activity)
* Optimizer: Adam (with 0.9 and 0.99 for beta1 and beta2)
* Epochs: 100
* Batch size: 256

Variable parameters (all the combinations were tested):

* Hidden Layers: [32, 16, 32], [64, 32, 16, 32, 64], [128, 64, 32, 64, 128]
> In the form of encoging_layer1, ..., encoging_layerN, encoded_layer, decoding_layer1, ..., decoding_layerN 
* Learning rates: 0.1, 0.01, 0.001

The models were built from all the combinations of the parameters. Each dataset was split into training and validation subsets. All the models were trained and then validated. For this experiment it was calculated a ranking order by the binary cross entropy loss over the validation subset, showing the best models for each dataset.

It was plotted the comparison between the original images and the decoded images and the encoded generated by the first encoding layer.

# Datasets
- [MNIST](http://yann.lecun.com/exdb/mnist/)
- [Fashion MNIST](https://www.kaggle.com/zalando-research/fashionmnist)

